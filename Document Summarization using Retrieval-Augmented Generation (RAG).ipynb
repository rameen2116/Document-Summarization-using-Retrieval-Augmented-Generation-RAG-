{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e172a5-d82f-4d8a-ae30-b412af2bec70",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "254f0c23-c0e6-4db5-83d9-b3ca7e6bc080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million \n",
      "($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. \n",
      "Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of \n",
      "gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast \n",
      "cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, \n",
      "suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian \n",
      "interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are \n",
      "things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a \n",
      "casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number \n",
      "one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. \n",
      "His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in \n",
      "an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five \n",
      "Potter films have been held in a trust fund which he has not been able to touch. Despite his growing \n",
      "fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to \n",
      "say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very hard not to go that way \n",
      "because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the \n",
      "Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the \n",
      "last two films. Watch I-Reporter give her review of Potter's latest » . There is life beyond Potter, \n",
      "however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and \n",
      "his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about \n",
      "four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured \n",
      "teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that \n",
      "he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Reuters. E-mail to a \n",
      "friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, \n",
      "rewritten, or redistributed. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import fitz  \n",
    "import markdown\n",
    "from pathlib import Path\n",
    "\n",
    "def load_document(file_path):\n",
    "    ext = Path(file_path).suffix.lower()\n",
    "    if ext == \".pdf\":\n",
    "        return load_pdf(file_path)\n",
    "    elif ext == \".txt\":\n",
    "        return Path(file_path).read_text()\n",
    "    elif ext == \".md\":\n",
    "        return markdown.markdown(Path(file_path).read_text())\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\\n\".join([page.get_text() for page in doc])\n",
    "    return text\n",
    "text = load_document(\"article2.txt\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d528f4-9d81-4378-80fa-8e1edca279b2",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4fb073c0-a38f-4f87-a89c-fba3eb36c752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONDON, England Letters -- Carry Other star Daniel Radcliffe gains access to a reported £20 million\n",
      "\n",
      "$41.1 million fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him.\n",
      "\n",
      "Daniel Radcliffe as Carry Other in \"Carry Other and the Order of the Phoenix\" To the disappointment of\n",
      "\n",
      "gossip colonists around the world, the young actor says he has no plans to written his cash away on fast\n",
      "\n",
      "cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18,\n",
      "\n",
      "suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian\n",
      "\n",
      "interview earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are\n",
      "\n",
      "things that cost about 10 pounds -- books and was and DVDs.\" It 18, Radcliffe will be able to gamble in a\n",
      "\n",
      "causing, buy a drink in a pub or see the horror film \"Hostel: Part of,\" currently six places below his number\n",
      "\n",
      "one movie on the of box office chart. Details of how he'll mark his landmarks birthday are under wraps.\n",
      "\n",
      "His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in\n",
      "\n",
      "an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five\n",
      "\n",
      "Other films have been held in a trust fund which he has not been able to touch. Despite his growing\n",
      "\n",
      "fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to\n",
      "\n",
      "say 'kid star goes off the rails,'\" he told reporter last month. \"But I try very hard not to go that way\n",
      "\n",
      "because it would be too easy for them.\" His latest outing as the boy wizard in \"Carry Other and the\n",
      "\n",
      "Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will deprive the role in the\n",
      "\n",
      "last two films. Watch I-Reporter give her review of Other's latest » . There is life beyond Other,\n",
      "\n",
      "however. The Londoners has filled a of movie called \"By Boy Back,\" about author Rudyard Filling and\n",
      "\n",
      "his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about\n",
      "\n",
      "four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured\n",
      "\n",
      "teenager in Peter Shafter's \"Quos.\" Meanwhile, he is braced for even closer media scrutiny now that\n",
      "\n",
      "he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Letters. E-mail to a\n",
      "\n",
      "friend . Copyright 2007 Letters. All rights reserved.His material may not be published, broadcast,\n",
      "\n",
      "rewritten, or redistribute.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "\n",
    "def preprocess_text(text: str, lang_filter='en', spellcheck=True) -> str:\n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Remove non-breaking spaces and other noise\n",
    "    text = text.replace(\"\\xa0\", \" \").replace(\"\\u200b\", \"\")\n",
    "\n",
    "    # Fix hyphenated line breaks (PDF artifacts)\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "    # Normalize line spacing\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "    # Remove markdown, HTML, boilerplate\n",
    "    text = re.sub(r'[#*\\[\\]\\(\\)]', '', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'Page \\d+ of \\d+', '', text)\n",
    "\n",
    "    # Split into paragraphs for filtering\n",
    "    paragraphs = [p.strip() for p in text.split('\\n') if p.strip()]\n",
    "    clean_paragraphs = []\n",
    "\n",
    "    for para in paragraphs:\n",
    "        try:\n",
    "            # Detect language\n",
    "            lang = detect(para)\n",
    "            if lang != lang_filter:\n",
    "                continue  # Skip non-English\n",
    "        except:\n",
    "            continue  # Skip detection errors\n",
    "\n",
    "        # Spellcheck if enabled\n",
    "        if spellcheck:\n",
    "            para = str(TextBlob(para).correct())\n",
    "\n",
    "        clean_paragraphs.append(para)\n",
    "\n",
    "    return '\\n\\n'.join(clean_paragraphs)\n",
    "\n",
    " \n",
    "text = preprocess_text(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7792ac-fa1c-420b-b355-f1c7c3b4e801",
   "metadata": {},
   "source": [
    "## semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "195ef776-40cb-40aa-b7e4-955308d79538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 (16 words):\n",
      "LONDON, England Letters -- Carry Other star Daniel Radcliffe gains access to a reported £20 million\n",
      "\n",
      "Chunk 2 (20 words):\n",
      "$41.1 million fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him.\n",
      "\n",
      "Chunk 3 (18 words):\n",
      "Daniel Radcliffe as Carry Other in \"Carry Other and the Order of the Phoenix\" To the disappointment of\n",
      "\n",
      "Chunk 4 (20 words):\n",
      "gossip colonists around the world, the young actor says he has no plans to written his cash away on fast\n",
      "\n",
      "Chunk 5 (21 words):\n",
      "cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18,\n",
      "\n",
      "Chunk 6 (15 words):\n",
      "suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian\n",
      "\n",
      "Chunk 7 (17 words):\n",
      "interview earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are\n",
      "\n",
      "Chunk 8 (22 words):\n",
      "things that cost about 10 pounds -- books and was and DVDs.\" It 18, Radcliffe will be able to gamble in a\n",
      "\n",
      "Chunk 9 (21 words):\n",
      "causing, buy a drink in a pub or see the horror film \"Hostel: Part of,\" currently six places below his number\n",
      "\n",
      "Chunk 10 (19 words):\n",
      "one movie on the of box office chart. Details of how he'll mark his landmarks birthday are under wraps.\n",
      "\n",
      "Chunk 11 (20 words):\n",
      "His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in\n",
      "\n",
      "Chunk 12 (17 words):\n",
      "an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five\n",
      "\n",
      "Chunk 13 (20 words):\n",
      "Other films have been held in a trust fund which he has not been able to touch. Despite his growing\n",
      "\n",
      "Chunk 14 (20 words):\n",
      "fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to\n",
      "\n",
      "Chunk 15 (22 words):\n",
      "say 'kid star goes off the rails,'\" he told reporter last month. \"But I try very hard not to go that way\n",
      "\n",
      "Chunk 16 (20 words):\n",
      "because it would be too easy for them.\" His latest outing as the boy wizard in \"Carry Other and the\n",
      "\n",
      "Chunk 17 (21 words):\n",
      "Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will deprive the role in the\n",
      "\n",
      "Chunk 18 (18 words):\n",
      "last two films. Watch I-Reporter give her review of Other's latest » . There is life beyond Other,\n",
      "\n",
      "Chunk 19 (17 words):\n",
      "however. The Londoners has filled a of movie called \"By Boy Back,\" about author Rudyard Filling and\n",
      "\n",
      "Chunk 20 (19 words):\n",
      "his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about\n",
      "\n",
      "Chunk 21 (17 words):\n",
      "four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured\n",
      "\n",
      "Chunk 22 (16 words):\n",
      "teenager in Peter Shafter's \"Quos.\" Meanwhile, he is braced for even closer media scrutiny now that\n",
      "\n",
      "Chunk 23 (22 words):\n",
      "he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Letters. E-mail to a\n",
      "\n",
      "Chunk 24 (14 words):\n",
      "friend . Copyright 2007 Letters. All rights reserved.His material may not be published, broadcast,\n",
      "\n",
      "Chunk 25 (3 words):\n",
      "rewritten, or redistribute.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "# Load spaCy's English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def semantic_chunk(text, max_tokens=200):\n",
    "    \"\"\"\n",
    "    Break the document into semantically meaningful chunks (paragraph/sentence level).\n",
    "    Each chunk contains up to `max_tokens` words (approximate).\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    chunks = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        sent_text = sent.text.strip()\n",
    "        sent_len = len(sent_text.split())\n",
    "\n",
    "        # If adding this sentence doesn't exceed max_tokens, add to current chunk\n",
    "        if current_length + sent_len <= max_tokens:\n",
    "            current_chunk.append(sent_text)\n",
    "            current_length += sent_len\n",
    "        else:\n",
    "            # Save current chunk and start a new one\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sent_text]\n",
    "            current_length = sent_len\n",
    "\n",
    "    # Add any remaining chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "def semantic_chunk_with_paragraphs(text, max_tokens=200):\n",
    "    paragraphs = text.split('\\n\\n')  # naive paragraph split\n",
    "    chunks = []\n",
    "\n",
    "    for para in paragraphs:\n",
    "        para = para.strip()\n",
    "        if not para:\n",
    "            continue\n",
    "\n",
    "        # Segment into sub-chunks using sentences if too long\n",
    "        if len(para.split()) > max_tokens:\n",
    "            chunks.extend(semantic_chunk(para, max_tokens))\n",
    "        else:\n",
    "            chunks.append(para)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "chunks = semantic_chunk_with_paragraphs(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1} ({len(chunk.split())} words):\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56e40c4-08cd-4107-9eec-410410bc5106",
   "metadata": {},
   "source": [
    "## Embedding & Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "58b7e160-12f3-4e06-a596-cb611288ad93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 39.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 (Score: 0.1899):\n",
      "friend . Copyright 2007 Letters. All rights reserved.His material may not be published, broadcast,\n",
      "\n",
      "Chunk 2 (Score: 0.1694):\n",
      "an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five\n",
      "\n",
      "Chunk 3 (Score: 0.1370):\n",
      "rewritten, or redistribute.\n",
      "\n",
      "Chunk 4 (Score: 0.1124):\n",
      "one movie on the of box office chart. Details of how he'll mark his landmarks birthday are under wraps.\n",
      "\n",
      "Chunk 5 (Score: 0.1082):\n",
      "teenager in Peter Shafter's \"Quos.\" Meanwhile, he is braced for even closer media scrutiny now that\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Example text\n",
    "\n",
    "\n",
    "# Chunk the text\n",
    "chunks = semantic_chunk_with_paragraphs(text)\n",
    "\n",
    "# Deduplicate chunks (fix repeated semantic results)\n",
    "unique_chunks = list(dict.fromkeys([chunk.strip() for chunk in chunks if chunk.strip()]))\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embedding_dim = 384\n",
    "\n",
    "# Store vectors\n",
    "chunk_texts = []\n",
    "chunk_embeddings = []\n",
    "\n",
    "print(\"Embedding chunks...\")\n",
    "for chunk in tqdm(unique_chunks):\n",
    "    try:\n",
    "        embedding = model.encode(chunk, convert_to_numpy=True)\n",
    "        if not np.isnan(embedding).any():\n",
    "            embedding = embedding / np.linalg.norm(embedding)  # normalize for cosine similarity\n",
    "            chunk_embeddings.append(embedding)\n",
    "            chunk_texts.append(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Convert to matrix\n",
    "embedding_matrix = np.vstack(chunk_embeddings).astype(\"float32\")\n",
    "\n",
    "# Build FAISS index (cosine similarity)\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "index.add(embedding_matrix)\n",
    "def search_chunks(query, top_k=5):\n",
    "    q_emb = model.encode(query, convert_to_numpy=True)\n",
    "    q_emb = q_emb / np.linalg.norm(q_emb)\n",
    "    q_emb = q_emb.astype(\"float32\").reshape(1, -1)\n",
    "    \n",
    "    distances, indices = index.search(q_emb, top_k)\n",
    "    return [(chunk_texts[i], distances[0][j]) for j, i in enumerate(indices[0])]\n",
    "query = \"Summarize this document\"\n",
    "results = search_chunks(query, top_k=5)\n",
    "\n",
    "for i, (chunk, score) in enumerate(results):\n",
    "    print(f\"Chunk {i+1} (Score: {score:.4f}):\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c91635-7123-489f-816a-bb0b59ad1bc5",
   "metadata": {},
   "source": [
    "## Summary Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "584ea00c-6e2f-40ed-8983-028607369e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 180, but your input_length is only 114. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=57)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=180) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Final Summary:\n",
      "\n",
      "Radcliffe's earnings from the first five rewritten, or redistribute . Details of how he'll mark his landmarks birthday are under wraps . He is braced for even closer media scrutiny .\n",
      "\n",
      "📊 Token Count: 111\n",
      "⏱️ Latency: 5.81 seconds\n",
      "📈 Similarity Score to Original Text: 0.7974\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import time\n",
    "\n",
    "# Load summarizer and tokenizer\n",
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Falconsai/text_summarization\")\n",
    "\n",
    "# Deduplicate top_chunks\n",
    "seen = set()\n",
    "deduped_chunks = []\n",
    "for text, _ in results:\n",
    "    cleaned = text.strip()\n",
    "    if cleaned not in seen:\n",
    "        deduped_chunks.append(cleaned)\n",
    "        seen.add(cleaned)\n",
    "\n",
    "# Combine deduplicated text\n",
    "combined_text = \"\\n\".join(deduped_chunks)\n",
    "\n",
    "# Token-aware truncation (limit to 1024 tokens for summarization model)\n",
    "tokens = tokenizer.tokenize(combined_text)\n",
    "token_count = len(tokens)\n",
    "if token_count > 1024:\n",
    "    tokens = tokens[:1024]\n",
    "    combined_text = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "# Summarize and measure latency\n",
    "start = time.time()\n",
    "summary = summarizer(combined_text, max_length=180, min_length=50, do_sample=False)[0]['summary_text']\n",
    "end = time.time()\n",
    "latency = end - start\n",
    "\n",
    "# Similarity score\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedder.encode([combined_text, summary], convert_to_tensor=True)\n",
    "similarity_score = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
    "\n",
    "# Output\n",
    "print(\"\\n🧠 Final Summary:\\n\")\n",
    "print(summary)\n",
    "print(f\"\\n📊 Token Count: {token_count}\")\n",
    "print(f\"⏱️ Latency: {latency:.2f} seconds\")\n",
    "print(f\"📈 Similarity Score to Original Text: {similarity_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "972861f8-d4eb-4466-b852-e3a8d719402f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 180, but your input_length is only 100. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Final Summary (BART):\n",
      "\n",
      "\"Hopefully none of you will be reading about it,\" Radcliffe says. Details of how he'll mark his landmarks birthday are under wraps. He is braced for even closer media scrutiny now that \"Quos\" is in cinemas.\n",
      "\n",
      "📊 Token Count: 69\n",
      "⏱️ Latency: 16.82 seconds\n",
      "📈 Similarity Score to Original Text: 0.7680\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import time\n",
    "\n",
    "# Load summarizer (free, works on CPU or GPU)\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Combine retrieved chunks\n",
    "combined_text = \"\\n\".join([text for text, _ in results])\n",
    "combined_text = combined_text[:3000]  \n",
    "\n",
    "# Token count approximation\n",
    "token_count = len(combined_text.split())\n",
    "\n",
    "# Summarize\n",
    "start = time.time()\n",
    "summary = summarizer(combined_text, max_length=180, min_length=50, do_sample=False)[0]['summary_text']\n",
    "end = time.time()\n",
    "latency = end - start\n",
    "\n",
    "# Similarity Score\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedder.encode([combined_text, summary], convert_to_tensor=True)\n",
    "similarity_score = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
    "\n",
    "# Output\n",
    "print(\"\\n🧠 Final Summary (BART):\\n\")\n",
    "print(summary)\n",
    "print(f\"\\n📊 Token Count: {token_count}\")\n",
    "print(f\"⏱️ Latency: {latency:.2f} seconds\")\n",
    "print(f\"📈 Similarity Score to Original Text: {similarity_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a020d6-b65e-4666-8706-5cbcea3afd1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
